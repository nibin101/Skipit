{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nibin101/Skipit/blob/main/intelligent_quiz_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3yWWtQ8cC4i"
      },
      "source": [
        "# STEP 0: Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR8CG3W7AdEz",
        "outputId": "a7874ae2-3d06-4e12-9cf6-5863fa091011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement google-genrativeai==0.8.5 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for google-genrativeai==0.8.5\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU google-generativeai google-ai-generativelanguage\\\n",
        "langgraph langchain langchain-google-genai openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvoFPl11cFhj"
      },
      "source": [
        "# STEP 1: Imports and secure API key input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QXaYSMSSCx9j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eusINPuVcLXp"
      },
      "source": [
        "# STEP 2: Initialize Gemini 1.5 Flash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xxCohNVcF7PB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9f736f9-9194-41fb-dd40-b5ae21f041ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the Gemini api key··········\n"
          ]
        }
      ],
      "source": [
        "os.environ['GOOGLE_API_KEY']=getpass.getpass(\"Enter the Gemini api key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W28u20SxcIiH"
      },
      "source": [
        "\n",
        "# Secure Gemini API Key input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "WnrEGf-yHM9Y"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm=ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash-latest\" ,temperature=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o07zbkLWcOVW"
      },
      "source": [
        "# STEP 3: Node to ask user for symptom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "uNGK9lY7INF1"
      },
      "outputs": [],
      "source": [
        "def get_subject_input(state: dict) -> dict:\n",
        "    \"\"\"Gets the subject input from the user.\"\"\"\n",
        "    subject = input(\"What subject do you want to learn about (physics, chemistry, or maths)? \")\n",
        "    state[\"subject\"] = subject.lower()\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KIzyYPVcRVm"
      },
      "source": [
        "# STEP 4: Node to classify the symptom"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fBIFZ18Pjqd9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "5ab0l3bHmpbG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def generate_mcqs(state: dict) -> dict:\n",
        "    \"\"\"Generates 5 MCQ questions on the given subject and stores them in the state.\"\"\"\n",
        "    subject = state[\"subject\"]\n",
        "    prompt = (\n",
        "        f\"Generate 5 multiple-choice questions about {subject}. \"\n",
        "        \"Each question should have 4 options (A, B, C, D) and indicate the correct answer. \"\n",
        "        \"Respond in JSON format as a list of objects, where each object has keys 'question', 'options', and 'answer'.\"\n",
        "        \"Do not include any ''' formatting\"\n",
        "    )\n",
        "    response = llm.invoke([HumanMessage(content=prompt)])\n",
        "    print(response.content)\n",
        "    try:\n",
        "        # Attempt to parse the content directly first\n",
        "        questions_json = json.loads(response.content)\n",
        "        state[\"questions\"] = [(q[\"question\"], q[\"options\"], q[\"answer\"]) for q in questions_json]\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Error decoding JSON response from the language model.\")\n",
        "        print(e)\n",
        "        # If direct parsing fails, try stripping potential markdown formatting\n",
        "        try:\n",
        "            cleaned_response = response.content.strip().replace(\"```json\\n\", \"\").replace(\"```\", \"\")\n",
        "            questions_json = json.loads(cleaned_response)\n",
        "            state[\"questions\"] = [(q[\"question\"], q[\"options\"], q[\"answer\"]) for q in questions_json]\n",
        "        except json.JSONDecodeError as e_cleaned:\n",
        "            print(\"Error decoding JSON response after cleaning.\")\n",
        "            print(e_cleaned)\n",
        "            state[\"questions\"] = []\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kESmxbnicWpS"
      },
      "source": [
        "# STEP 5: Router logic to route to the correct node\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0d90qeH3nbKl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4831b7ba-ab55-4fe6-d5a0-ca9552b76d45"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error decoding JSON response from the language model.\n",
            "Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "Here are the questions:\n",
            "\n",
            "Question 1: Which organ is responsible for filtering waste from the blood?\n",
            "A. Heart\n",
            "B. Lungs\n",
            "C. Kidneys\n",
            "D. Liver\n",
            "Correct!\n",
            "\n",
            "Question 2: What is the largest organ in the human body?\n",
            "A. Brain\n",
            "B. Heart\n",
            "C. Liver\n",
            "D. Skin\n",
            "Correct!\n",
            "\n",
            "Question 3: Which type of blood cell is responsible for carrying oxygen?\n",
            "A. Platelets\n",
            "B. White blood cells\n",
            "C. Red blood cells\n",
            "D. Plasma\n",
            "Correct!\n",
            "\n",
            "Question 4: What is the name of the long bone in the upper arm?\n",
            "A. Tibia\n",
            "B. Fibula\n",
            "C. Femur\n",
            "D. Humerus\n",
            "Incorrect. The correct answer is D.\n",
            "\n",
            "Question 5: Which system is responsible for the body's movement?\n",
            "A. Respiratory system\n",
            "B. Digestive system\n",
            "C. Muscular system\n",
            "D. Nervous system\n",
            "Correct!\n",
            "\n",
            "Your score: 4/5\n",
            "\n",
            "Here's a summary of the topics you answered incorrectly:\n",
            "\n",
            "This question pertains to human anatomy, specifically the skeletal system.  It asks for the identification of a specific long bone located in the upper limb, the region of the body extending from the shoulder to the elbow.  Understanding the names and locations of bones is fundamental to anatomical knowledge.\n",
            "Okay, I will generate questions about maths next.\n",
            "Error decoding JSON response from the language model.\n",
            "Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "Here are the questions:\n",
            "\n",
            "Question 1: What is the value of 5 + 3 * 2?\n",
            "A. 11\n",
            "B. 16\n",
            "C. 25\n",
            "D. 10\n",
            "Correct!\n",
            "\n",
            "Question 2: What is the square root of 144?\n",
            "A. 11\n",
            "B. 12\n",
            "C. 13\n",
            "D. 14\n",
            "Incorrect. The correct answer is B.\n",
            "\n",
            "Question 3: If x + 7 = 12, what is the value of x?\n",
            "A. 5\n",
            "B. 19\n",
            "C. 7\n",
            "D. -5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4246521171.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4246521171.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{letter}. {cleaned_option_text}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0muser_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your answer (enter the option letter, e.g., A): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;31m# Check if the answer is correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "def get_subject_input(state: dict) -> dict:\n",
        "    \"\"\"Gets the subject input from the user.\"\"\"\n",
        "    subject = input(\"What subject do you want to learn about and be quizzed on? \")\n",
        "    state[\"subject\"] = subject.lower()\n",
        "    return state\n",
        "\n",
        "def generate_mcqs(state: dict) -> dict:\n",
        "    \"\"\"Generates 5 MCQ questions on the given subject and stores them in the state.\"\"\"\n",
        "    subject = state[\"subject\"]\n",
        "    prompt = (\n",
        "        f\"Generate 5 multiple-choice questions about {subject}. \"\n",
        "        \"Each question should have 4 options (A, B, C, D) and indicate the correct answer. \"\n",
        "        \"Respond in JSON format as a list of objects, where each object has keys 'question', 'options', and 'answer'.\"\n",
        "        \"Do not include any ''' formatting\"\n",
        "    )\n",
        "    # Ensure llm is defined or imported if needed here\n",
        "    # response = llm.invoke([HumanMessage(content=prompt)]) # Uncomment and ensure llm is available\n",
        "\n",
        "    # Mock response for demonstration if llm is not available\n",
        "    # response_content = \"\"\"\n",
        "    # [\n",
        "    #     {\n",
        "    #         \"question\": \"What is the chemical symbol for water?\",\n",
        "    #         \"options\": {\"A\": \"O2\", \"B\": \"H2O\", \"C\": \"CO2\", \"D\": \"NaCl\"},\n",
        "    #         \"answer\": \"B\"\n",
        "    #     },\n",
        "    #     {\n",
        "    #         \"question\": \"What is the capital of France?\",\n",
        "    #         \"options\": {\"A\": \"Berlin\", \"B\": \"Madrid\", \"C\": \"Paris\", \"D\": \"Rome\"},\n",
        "    #         \"answer\": \"C\"\n",
        "    #     },\n",
        "    #     {\n",
        "    #         \"question\": \"What is the largest planet in our solar system?\",\n",
        "    #         \"options\": {\"A\": \"Earth\", \"B\": \"Mars\", \"C\": \"Jupiter\", \"D\": \"Venus\"},\n",
        "    #         \"answer\": \"C\"\n",
        "    #     },\n",
        "    #     {\n",
        "    #         \"question\": \"What is the square root of 64?\",\n",
        "    #         \"options\": {\"A\": \"6\", \"B\": \"7\", \"C\": \"8\", \"D\": \"9\"},\n",
        "    #         \"answer\": \"C\"\n",
        "    #     },\n",
        "    #     {\n",
        "    #         \"question\": \"Who wrote 'Romeo and Juliet'?\",\n",
        "    #         \"options\": {\"A\": \"Charles Dickens\", \"B\": \"William Shakespeare\", \"C\": \"Jane Austen\", \"D\": \"Leo Tolstoy\"},\n",
        "    #         \"answer\": \"B\"\n",
        "    #     }\n",
        "    # ]\n",
        "    # \"\"\"\n",
        "    # Replace the mock response with the actual LLM call when available\n",
        "    try:\n",
        "        llm=ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash-latest\" ,temperature=0.2)\n",
        "        response = llm.invoke([HumanMessage(content=prompt)])\n",
        "        questions_json = json.loads(response.content)\n",
        "        state[\"questions\"] = [(q[\"question\"], q[\"options\"], q[\"answer\"]) for q in questions_json]\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Error decoding JSON response from the language model.\")\n",
        "        print(e)\n",
        "        try:\n",
        "            cleaned_response = response.content.strip().replace(\"```json\\n\", \"\").replace(\"```\", \"\")\n",
        "            questions_json = json.loads(cleaned_response)\n",
        "            state[\"questions\"] = [(q[\"question\"], q[\"options\"], q[\"answer\"]) for q in questions_json]\n",
        "        except json.JSONDecodeError as e_cleaned:\n",
        "            print(\"Error decoding JSON response after cleaning.\")\n",
        "            print(e_cleaned)\n",
        "            state[\"questions\"] = []\n",
        "\n",
        "    # Using mock response for now\n",
        "    # try:\n",
        "    #     questions_json = json.loads(response_content)\n",
        "    #     state[\"questions\"] = [(q[\"question\"], q[\"options\"], q[\"answer\"]) for q in questions_json]\n",
        "    # except json.JSONDecodeError as e:\n",
        "    #     print(\"Error decoding mock JSON response.\")\n",
        "    #     print(e)\n",
        "    #     state[\"questions\"] = []\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "def main():\n",
        "    state = {}\n",
        "    state = get_subject_input(state)\n",
        "\n",
        "    while True:\n",
        "        state = generate_mcqs(state)\n",
        "\n",
        "        correct_answers = 0\n",
        "        total_questions = len(state.get(\"questions\", []))\n",
        "        answered_correctly = []\n",
        "        answered_incorrectly = []\n",
        "\n",
        "        # Display MCQs and get user answers\n",
        "        if state.get(\"questions\"):\n",
        "            print(\"\\nHere are the questions:\")\n",
        "            for i, (question, options, answer) in enumerate(state[\"questions\"]):\n",
        "                # Remove <sup> tags from the question\n",
        "                cleaned_question = re.sub(r'<sup[^>]*>.*?</sup>', '', question)\n",
        "                print(f\"\\nQuestion {i+1}: {cleaned_question}\")\n",
        "                # Iterate through the options dictionary to display letter and text\n",
        "                for letter, option_text in options.items():\n",
        "                    # Remove <sup> tags from the options\n",
        "                    cleaned_option_text = re.sub(r'<sup[^>]*>.*?</sup>', '', option_text)\n",
        "                    print(f\"{letter}. {cleaned_option_text}\")\n",
        "\n",
        "                user_answer = input(\"Your answer (enter the option letter, e.g., A): \").upper()\n",
        "\n",
        "                # Check if the answer is correct\n",
        "                # Simplified check based on the dictionary answer\n",
        "                correct_option_letter = answer\n",
        "\n",
        "\n",
        "                if user_answer == correct_option_letter:\n",
        "                    print(\"Correct!\")\n",
        "                    correct_answers += 1\n",
        "                    answered_correctly.append(question)\n",
        "                else:\n",
        "                    print(f\"Incorrect. The correct answer is {correct_option_letter}.\")\n",
        "                    answered_incorrectly.append(question)\n",
        "        else:\n",
        "            print(\"No questions were generated.\")\n",
        "\n",
        "        # Print the score\n",
        "        if total_questions > 0:\n",
        "            print(f\"\\nYour score: {correct_answers}/{total_questions}\")\n",
        "\n",
        "            # Use LLM to summarize topics\n",
        "            if answered_incorrectly:\n",
        "                print(\"\\nHere's a summary of the topics you answered incorrectly:\")\n",
        "                for incorrect_question in answered_incorrectly:\n",
        "                    incorrect_topic_prompt = f\"Explain the topic of the following question in a short paragraph:\\n{incorrect_question}\"\n",
        "                    llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash-latest\" ,temperature=0.2)\n",
        "                    incorrect_topic_response = llm.invoke([HumanMessage(content=incorrect_topic_prompt)])\n",
        "                    cleaned_incorrect_topic_explanation = incorrect_topic_response.content.replace('**', '')\n",
        "                    print(f\"\\n{cleaned_incorrect_topic_explanation}\")\n",
        "\n",
        "            # Ask for the next topic\n",
        "            next_subject = input(\"\\nWhich topic would you like to be quizzed on next? (or type 'quit' to exit) \")\n",
        "            if next_subject.lower() == 'quit':\n",
        "                break\n",
        "            state[\"subject\"] = next_subject.lower()\n",
        "            print(f\"Okay, I will generate questions about {next_subject} next.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PEJToOxQj-mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZuR4VGscake"
      },
      "source": [
        "# STEP 6: Category-specific response nodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUBdWPVWQjRA"
      },
      "outputs": [],
      "source": [
        "def general_node(stae:dict)->dict\n",
        "  state[\"answer\"]=f\"{state['state']} sems to be general .you are being directed to cunsulting a general doctor\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1ek0SwzcehS"
      },
      "source": [
        "# STEP 7: Build LangGraph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APpX2iWSl_MR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbAgCeZYci5k"
      },
      "source": [
        "# STEP 8: Compile and invoke the graph\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}